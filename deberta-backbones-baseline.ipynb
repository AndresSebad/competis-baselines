{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "033d59b4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:28.523456Z",
     "iopub.status.busy": "2024-02-08T00:57:28.523129Z",
     "iopub.status.idle": "2024-02-08T00:57:35.448570Z",
     "shell.execute_reply": "2024-02-08T00:57:35.447748Z"
    },
    "papermill": {
     "duration": 6.934375,
     "end_time": "2024-02-08T00:57:35.451031",
     "exception": false,
     "start_time": "2024-02-08T00:57:28.516656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13a1a595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:35.462607Z",
     "iopub.status.busy": "2024-02-08T00:57:35.462183Z",
     "iopub.status.idle": "2024-02-08T00:57:35.670517Z",
     "shell.execute_reply": "2024-02-08T00:57:35.669513Z"
    },
    "papermill": {
     "duration": 0.216405,
     "end_time": "2024-02-08T00:57:35.672751",
     "exception": false,
     "start_time": "2024-02-08T00:57:35.456346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n",
    "\n",
    "prompts_train = pd.read_csv(data_path + \"prompts_train.csv\")\n",
    "prompts_test = pd.read_csv(data_path + \"prompts_test.csv\")\n",
    "summaries_train = pd.read_csv(data_path + \"summaries_train.csv\")\n",
    "summaries_test = pd.read_csv(data_path + \"summaries_test.csv\")\n",
    "sample_submission = pd.read_csv(data_path + \"sample_submission.csv\")\n",
    "\n",
    "train = summaries_train.merge(prompts_train, on=\"prompt_id\")\n",
    "test = summaries_test.merge(prompts_test, on=\"prompt_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76157438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:35.683754Z",
     "iopub.status.busy": "2024-02-08T00:57:35.683466Z",
     "iopub.status.idle": "2024-02-08T00:57:51.394425Z",
     "shell.execute_reply": "2024-02-08T00:57:51.393520Z"
    },
    "papermill": {
     "duration": 15.718639,
     "end_time": "2024-02-08T00:57:51.396441",
     "exception": false,
     "start_time": "2024-02-08T00:57:35.677802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/debertav3base and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/debertav3base\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/input/debertav3base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b739ed9",
   "metadata": {
    "papermill": {
     "duration": 0.00486,
     "end_time": "2024-02-08T00:57:51.406477",
     "exception": false,
     "start_time": "2024-02-08T00:57:51.401617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2450e9bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:51.417573Z",
     "iopub.status.busy": "2024-02-08T00:57:51.417283Z",
     "iopub.status.idle": "2024-02-08T00:57:51.702412Z",
     "shell.execute_reply": "2024-02-08T00:57:51.701370Z"
    },
    "papermill": {
     "duration": 0.293277,
     "end_time": "2024-02-08T00:57:51.704662",
     "exception": false,
     "start_time": "2024-02-08T00:57:51.411385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Engeneering\n",
    "# Text Word Count\n",
    "train['text_word_count'] = train['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "train['summary_text_word_count'] = train['prompt_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "train['summary_to_original_ratio'] = train['text_word_count'] / train['summary_text_word_count']\n",
    "\n",
    "# Test\n",
    "test['text_word_count'] = test['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "test['summary_text_word_count'] = test['prompt_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "test['summary_to_original_ratio'] = test['text_word_count'] / test['summary_text_word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d5b6693",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:51.716340Z",
     "iopub.status.busy": "2024-02-08T00:57:51.715997Z",
     "iopub.status.idle": "2024-02-08T00:57:51.720304Z",
     "shell.execute_reply": "2024-02-08T00:57:51.719585Z"
    },
    "papermill": {
     "duration": 0.012227,
     "end_time": "2024-02-08T00:57:51.722188",
     "exception": false,
     "start_time": "2024-02-08T00:57:51.709961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from transformers import pipeline\n",
    "# Sentiment Analysis\n",
    "# Obtendremos la diferencia en el tono entre el resumen y el texto original\n",
    "\n",
    "#sentiment_analysis = pipeline('sentiment-analysis') # device = 0, con CUDA\n",
    "\n",
    "#def get_sentiment(text):\n",
    "    #result = sentiment_analysis(text)\n",
    "    #if result[0]['label'] = 'POSITIVE':\n",
    "        #return result[0]['score']\n",
    "    #else:\n",
    "        #return -result[0]['score']\n",
    "    \n",
    "#train['sentiment_text'] = train['text'].apply(get_sentiment)\n",
    "#train['sentiment_prompt'] = train['prompt_text'].apply(get_sentiment)\n",
    "#train['sentiment_difference'] = train['sentiment_prompt'] - train['sentiment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f666d5f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:51.733672Z",
     "iopub.status.busy": "2024-02-08T00:57:51.733412Z",
     "iopub.status.idle": "2024-02-08T00:57:51.742947Z",
     "shell.execute_reply": "2024-02-08T00:57:51.742200Z"
    },
    "papermill": {
     "duration": 0.017387,
     "end_time": "2024-02-08T00:57:51.744714",
     "exception": false,
     "start_time": "2024-02-08T00:57:51.727327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetSummary(Dataset):\n",
    "    def __init__(self,data,tokenizer):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.features = self.data[['summary_to_original_ratio', 'text_word_count']].values\n",
    "        self.text = self.data[\"text\"].tolist()\n",
    "        self.text = self.get_token(self.text)                \n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        input_ids = self.text['input_ids'][index]\n",
    "        attention_mask = self.text['attention_mask'][index]\n",
    "        \n",
    "        if 'content' not in self.data.columns:\n",
    "            return {'input_ids':input_ids,\n",
    "                   'attention_mask':attention_mask,\n",
    "                   'features' : torch.tensor(self.features[index], dtype=torch.float32)}\n",
    "        else:\n",
    "            content = self.data[\"content\"].tolist()[index]\n",
    "            wording = self.data[\"wording\"].tolist()[index]\n",
    "\n",
    "            return {'input_ids' : input_ids,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'content' : content,\n",
    "                    'wording' : wording,\n",
    "                    'features' : torch.tensor(self.features[index], dtype=torch.float32)}\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data['text'])\n",
    "    \n",
    "    def get_token(self,text):\n",
    "        return self.tokenizer.batch_encode_plus(text,\n",
    "                                         padding=True,\n",
    "                                         truncation=True,\n",
    "                                         max_length=512,\n",
    "                                         return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e721118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:51.755677Z",
     "iopub.status.busy": "2024-02-08T00:57:51.755416Z",
     "iopub.status.idle": "2024-02-08T00:57:56.669946Z",
     "shell.execute_reply": "2024-02-08T00:57:56.669178Z"
    },
    "papermill": {
     "duration": 4.922564,
     "end_time": "2024-02-08T00:57:56.672226",
     "exception": false,
     "start_time": "2024-02-08T00:57:51.749662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "\n",
    "target = ['content','wording']\n",
    "datas = ['text', 'text_word_count', 'summary_to_original_ratio']\n",
    "\n",
    "data = train.loc[:,datas]\n",
    "label = train.loc[:,target]\n",
    "\n",
    "train_data,val_data,train_label,val_label = train_test_split(data,label,test_size=0.2,random_state=42)\n",
    "\n",
    "train_data = pd.concat([train_data,train_label],axis=1)\n",
    "val_data = pd.concat([val_data,val_label],axis=1)\n",
    "\n",
    "train_dataset = DatasetSummary(train_data,tokenizer)\n",
    "train_loader = DataLoader(train_dataset,shuffle=False,batch_size=batch_size)\n",
    "    \n",
    "val_dataset = DatasetSummary(val_data,tokenizer)\n",
    "val_loader = DataLoader(val_dataset,shuffle=False,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "695a9ee9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:56.683836Z",
     "iopub.status.busy": "2024-02-08T00:57:56.683546Z",
     "iopub.status.idle": "2024-02-08T00:57:56.735294Z",
     "shell.execute_reply": "2024-02-08T00:57:56.734384Z"
    },
    "papermill": {
     "duration": 0.059643,
     "end_time": "2024-02-08T00:57:56.737214",
     "exception": false,
     "start_time": "2024-02-08T00:57:56.677571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 17246,   262,  ...,     0,     0,     0],\n",
       "         [    1,   279,   728,  ...,     0,     0,     0],\n",
       "         [    1,   279, 52789,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    1,   450,   338,  ...,     0,     0,     0],\n",
       "         [    1,   816, 11647,  ...,     0,     0,     0],\n",
       "         [    1,  9339,   728,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'content': tensor([-0.9106, -0.6643,  0.3884,  0.7507, -1.3325,  0.2057, -1.6385, -0.7826,\n",
       "         -0.3933,  0.3884,  1.5797,  0.2915], dtype=torch.float64),\n",
       " 'wording': tensor([-0.0818, -0.5107, -0.7180, -0.1295, -1.0056,  0.3805, -0.9120, -0.2460,\n",
       "          0.6271, -0.7180,  1.7133,  1.0426], dtype=torch.float64),\n",
       " 'features': tensor([[7.1429e-02, 6.9000e+01],\n",
       "         [5.8182e-02, 3.2000e+01],\n",
       "         [1.9455e-01, 1.0700e+02],\n",
       "         [1.4073e-01, 8.5000e+01],\n",
       "         [6.1818e-02, 3.4000e+01],\n",
       "         [1.7818e-01, 9.8000e+01],\n",
       "         [6.0000e-02, 3.3000e+01],\n",
       "         [4.8658e-02, 2.9000e+01],\n",
       "         [8.5455e-02, 4.7000e+01],\n",
       "         [9.2133e-02, 8.9000e+01],\n",
       "         [1.8377e-01, 1.1100e+02],\n",
       "         [1.0000e-01, 5.5000e+01]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bebaf837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:56.749137Z",
     "iopub.status.busy": "2024-02-08T00:57:56.748804Z",
     "iopub.status.idle": "2024-02-08T00:57:56.757238Z",
     "shell.execute_reply": "2024-02-08T00:57:56.756457Z"
    },
    "papermill": {
     "duration": 0.016494,
     "end_time": "2024-02-08T00:57:56.759067",
     "exception": false,
     "start_time": "2024-02-08T00:57:56.742573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Deberta(nn.Module):\n",
    "    def __init__(self, deberta, feature_dim):\n",
    "        super(Deberta, self).__init__()\n",
    "        \n",
    "        self.deberta = deberta\n",
    "        \n",
    "        self.feature_backbone = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2 + 32, 64),  # Ajustando la dimensión de entrada según el output de feature_backbone y deberta\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, features):\n",
    "        x = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = x[0].type(torch.float32)\n",
    "        feature = self.feature_backbone(features)\n",
    "        \n",
    "        # Fusionamos\n",
    "        combined = torch.cat([x, feature], dim=1)\n",
    "        combined = self.classifier(combined)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e65ce05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:56.770518Z",
     "iopub.status.busy": "2024-02-08T00:57:56.770246Z",
     "iopub.status.idle": "2024-02-08T00:57:57.175332Z",
     "shell.execute_reply": "2024-02-08T00:57:57.174333Z"
    },
    "papermill": {
     "duration": 0.413528,
     "end_time": "2024-02-08T00:57:57.177773",
     "exception": false,
     "start_time": "2024-02-08T00:57:56.764245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Deberta(model, 2).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(),lr=1.5e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97aaeb38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:57.189645Z",
     "iopub.status.busy": "2024-02-08T00:57:57.189345Z",
     "iopub.status.idle": "2024-02-08T00:57:57.194942Z",
     "shell.execute_reply": "2024-02-08T00:57:57.194079Z"
    },
    "papermill": {
     "duration": 0.013616,
     "end_time": "2024-02-08T00:57:57.196824",
     "exception": false,
     "start_time": "2024-02-08T00:57:57.183208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#epochs = 30\n",
    "\n",
    "#model.train()\n",
    "\n",
    "#for epoch in range(epochs):\n",
    "    #running_loss = 0\n",
    "    #step = 0\n",
    "    #for data in train_loader:\n",
    "        #input_ids = data['input_ids'].to(device)\n",
    "        #attention_mask = data['attention_mask'].to(device)\n",
    "        #content = data['content'].type(torch.float32).to(device)\n",
    "        #wording = data['wording'].type(torch.float32).to(device)\n",
    "        #features = data['features'].type(torch.float32).to(device)\n",
    "\n",
    "        #optim.zero_grad()\n",
    "        #outputs = model(input_ids, attention_mask, features)\n",
    "        #loss = criterion(outputs[:,0],content) + criterion(outputs[:,1],wording)\n",
    "        #loss.backward()\n",
    "        #optim.step()\n",
    "        #if step % 500 == 0:\n",
    "            #print(\"Epoch {}, Step {}, Loss: {}\".format(epoch+1, step, loss.item()))\n",
    "\n",
    "        #running_loss += loss.item()\n",
    "        #step = step + 1\n",
    "\n",
    "    #print(f\"Epoch {epoch+1} Loss: {running_loss / len(train_loader)}\")\n",
    "        \n",
    "    #model.eval()\n",
    "    #with torch.no_grad():\n",
    "        #val_loss = 0.0\n",
    "        #step = 0\n",
    "        #for data in val_loader:\n",
    "            #input_ids = data['input_ids'].to(device)\n",
    "            #attention_mask = data['attention_mask'].to(device)\n",
    "            #content = data['content'].type(torch.float32).to(device)\n",
    "            #wording = data['wording'].type(torch.float32).to(device)\n",
    "            #features = data['features'].type(torch.float32).to(device)\n",
    "            \n",
    "            #outputs = model(input_ids,attention_mask, features)\n",
    "            #val_loss+=criterion(outputs[:,0],content)+criterion(outputs[:,1],wording)\n",
    "                \n",
    "        #print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n",
    "    #model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "344acdb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:57.208196Z",
     "iopub.status.busy": "2024-02-08T00:57:57.207903Z",
     "iopub.status.idle": "2024-02-08T00:57:57.211924Z",
     "shell.execute_reply": "2024-02-08T00:57:57.211096Z"
    },
    "papermill": {
     "duration": 0.011895,
     "end_time": "2024-02-08T00:57:57.213830",
     "exception": false,
     "start_time": "2024-02-08T00:57:57.201935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.eval()\n",
    "#predict = []\n",
    "\n",
    "#test_dataset = DatasetSummary(test,tokenizer)\n",
    "#test_loader = DataLoader(test_dataset,shuffle=False,batch_size=batch_size)\n",
    "\n",
    "#with torch.no_grad():\n",
    "    #for data in test_loader:\n",
    "        #input_ids = data['input_ids'].to(device)\n",
    "        #attention_mask = data['attention_mask'].to(device)\n",
    "        #features = data['features'].to(device)\n",
    "        \n",
    "        #outputs = model(input_ids,attention_mask,features)\n",
    "        #predict.extend(outputs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "244137be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:57.225305Z",
     "iopub.status.busy": "2024-02-08T00:57:57.224789Z",
     "iopub.status.idle": "2024-02-08T00:57:57.228578Z",
     "shell.execute_reply": "2024-02-08T00:57:57.227739Z"
    },
    "papermill": {
     "duration": 0.01152,
     "end_time": "2024-02-08T00:57:57.230448",
     "exception": false,
     "start_time": "2024-02-08T00:57:57.218928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#submission = pd.DataFrame({\n",
    "    #'student_id':test['student_id'],\n",
    "    #'content':[pred[0] for pred in predict],\n",
    "    #'wording':[pred[1] for pred in predict]\n",
    "#}) \n",
    "#submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb45a0a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T00:57:57.241704Z",
     "iopub.status.busy": "2024-02-08T00:57:57.241439Z",
     "iopub.status.idle": "2024-02-08T00:57:57.245171Z",
     "shell.execute_reply": "2024-02-08T00:57:57.244320Z"
    },
    "papermill": {
     "duration": 0.011377,
     "end_time": "2024-02-08T00:57:57.247014",
     "exception": false,
     "start_time": "2024-02-08T00:57:57.235637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6201832,
     "sourceId": 53482,
     "sourceType": "competition"
    },
    {
     "datasetId": 2210196,
     "sourceId": 3693646,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35.556908,
   "end_time": "2024-02-08T00:58:00.667715",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-08T00:57:25.110807",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
